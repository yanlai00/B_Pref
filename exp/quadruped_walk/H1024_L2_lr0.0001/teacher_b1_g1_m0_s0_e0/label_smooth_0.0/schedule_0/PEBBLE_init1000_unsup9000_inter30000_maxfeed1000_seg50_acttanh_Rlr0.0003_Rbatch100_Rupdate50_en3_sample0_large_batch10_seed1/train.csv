actor_entropy,actor_loss,actor_target_entropy,alpha_loss,alpha_value,batch_reward,critic_entropy,critic_entropy_max,critic_entropy_min,critic_loss,critic_norm_entropy,critic_norm_entropy_max,critic_norm_entropy_min,duration,episode,episode_reward,labeled_feedback,step,total_feedback,true_episode_reward
5.752079810652587,-12.482712810536524,-12.0,1.6932929706227433,0.09525589518868052,-0.0061427292143539135,18.981512226261295,83.66264348989492,13.751372409893108,1.8816166369764655,4.77611502703723,20.884733997188413,3.461448244385056,27.13267493247986,2.0,-12.032889702239117,0.0,2000,0.0,476.7046299219754
0.8453733573406935,-23.966288682937623,-12.0,1.126085395872593,0.08737580602616071,-0.0014754724047525087,19.476716815948485,109.73961153793336,13.140837471961975,1.2543251389861108,4.366685774564743,24.238624165534972,2.9486690027713776,34.44698524475098,3.0,6.199720251217514,0.0,3000,0.0,6.425190354544308
